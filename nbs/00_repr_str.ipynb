{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¾ View as a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp repr_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import os\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# For testing, I want to see 8 CPU devices.\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import jax, jax.numpy as jnp\n",
    "\n",
    "from lovely_numpy import np_to_str_common, pretty_str, sparse_join, ansi_color, in_debugger, bytes_to_human\n",
    "from lovely_numpy import config as lnp_config\n",
    "\n",
    "from lovely_jax.utils.config import get_config, config\n",
    "from lovely_jax.utils.misc import is_cpu, test_array_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "key = jax.random.PRNGKey(0)\n",
    "randoms = jax.random.normal(key, (100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spicy = (randoms[:12].at[0].mul(10000)\n",
    "                    .at[1].divide(10000)\n",
    "                    .at[3].set(float('inf'))\n",
    "                    .at[4].set(float('-inf'))\n",
    "                    .at[5].set(float('nan'))\n",
    "                    .reshape((2,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "dtnames =   {   \"float16\": \"f16\",\n",
    "                \"float32\": \"\", # Default dtype in jax\n",
    "                \"float64\": \"f64\",\n",
    "                \"bfloat16\": \"bf16\",\n",
    "                \"uint8\": \"u8\",\n",
    "                \"uint16\": \"u16\",\n",
    "                \"uint32\": \"u32\",\n",
    "                \"uint64\": \"u64\",\n",
    "                \"int8\": \"i8\",\n",
    "                \"int16\": \"i16\",\n",
    "                \"int32\": \"i32\",\n",
    "                \"int64\": \"i64\",\n",
    "            }\n",
    "\n",
    "def short_dtype(x: jax.Array) -> str:\n",
    "    return dtnames.get(x.dtype.name, str(x.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_eq(short_dtype(jnp.array(1., dtype=jnp.bfloat16)), \"bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def plain_repr(x: jax.Array):\n",
    "    \"Pick the right function to get a plain repr\"\n",
    "    # assert isinstance(x, np.ndarray), f\"expected np.ndarray but got {type(x)}\" # Could be a sub-class.\n",
    "    return x._plain_repr() if hasattr(x, \"_plain_repr\") else repr(x)\n",
    "\n",
    "# def plain_str(x: torch.Tensor):\n",
    "#     \"Pick the right function to get a plain str.\"\n",
    "#     # assert isinstance(x, np.ndarray), f\"expected np.ndarray but got {type(x)}\"\n",
    "#     return x._plain_str() if hasattr(type(x), \"_plain_str\") else str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def is_nasty(x: jax.Array):\n",
    "    \"\"\"Return true of any `x` values are inf or nan\"\"\"\n",
    "\n",
    "    if x.size == 0: return False # min/max don't like zero-lenght arrays\n",
    "\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "\n",
    "    return jnp.isnan(x_min) or jnp.isinf(x_min) or jnp.isinf(x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "test_eq(is_nasty(jnp.array([1, 2, float(\"nan\")])), True)\n",
    "test_eq(is_nasty(jnp.array([1, 2, float(\"inf\")])), True)\n",
    "test_eq(is_nasty(jnp.array([1, 2, 3])), False)\n",
    "test_eq(is_nasty(jnp.array([])), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "def format_sharding(sharding) -> str:\n",
    "    \"\"\"Format sharding information in a compact, informative way.\"\"\"\n",
    "    from jax.sharding import SingleDeviceSharding, NamedSharding\n",
    "\n",
    "    devices = sorted(sharding.device_set, key=lambda d: d.id)\n",
    "    platform = devices[0].platform\n",
    "\n",
    "    if len(devices) == 1:\n",
    "        return f\"{platform}:{devices[0].id}\"\n",
    "\n",
    "    # Format device range\n",
    "    dev_ids = [d.id for d in devices]\n",
    "    if len(set(dev_ids)) > 2:\n",
    "        dev_range = f\"{min(dev_ids)}-{max(dev_ids)}\"\n",
    "    else:\n",
    "        dev_range = \",\".join(map(str, dev_ids))\n",
    "\n",
    "    # Add sharding type info\n",
    "    if isinstance(sharding, SingleDeviceSharding):\n",
    "        shard_info = \"\"\n",
    "    elif isinstance(sharding, NamedSharding):\n",
    "        # Format PartitionSpec compactly: P('x', 'y') -> S[x,y], P('x', None) -> S[x,Â·]\n",
    "        spec_str = str(sharding.spec)\n",
    "        # Extract the content from PartitionSpec(...)\n",
    "        if \"PartitionSpec\" in spec_str:\n",
    "            spec_str = spec_str.replace(\"PartitionSpec(\", \"\").rstrip(\")\")\n",
    "        # Clean up the formatting\n",
    "        spec_str = spec_str.replace(\"'\", \"\").replace(\", \", \",\").replace(\"None\", \"Â·\")\n",
    "\n",
    "        # Add mesh shape for multi-dimensional meshes\n",
    "        # mesh.shape is an OrderedDict with axis_names as keys\n",
    "        mesh_shape = sharding.mesh.shape\n",
    "        if len(mesh_shape) > 1:\n",
    "            # Get the axis names from the mesh in order\n",
    "            axis_names = list(mesh_shape.keys())\n",
    "            # Build shape string in the order of axis names\n",
    "            mesh_str = \"Ã—\".join(str(mesh_shape[name]) for name in axis_names)\n",
    "            shard_info = f\"S[{spec_str}] {mesh_str} \"\n",
    "        else:\n",
    "            shard_info = f\"S[{spec_str}] \"\n",
    "    else:\n",
    "        shard_info = f\"{type(sharding).__name__} \"\n",
    "\n",
    "    return f\"{shard_info}{platform}:{dev_range}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def jax_to_str_common(x: jax.Array,  # Input\n",
    "                        color=True,                     # ANSI color highlighting\n",
    "                        ddof=0):                        # For \"std\" unbiasing\n",
    "\n",
    "    if x.size == 0:\n",
    "        return ansi_color(\"empty\", \"grey\", color)\n",
    "\n",
    "    zeros = ansi_color(\"all_zeros\", \"grey\", color) if jnp.equal(x, 0.).all() and x.size > 1 else None\n",
    "\n",
    "    summary = None\n",
    "    if not zeros and x.ndim > 0:\n",
    "        minmax = f\"xâˆˆ[{pretty_str(x.min())}, {pretty_str(x.max())}]\" if x.size > 2 else None\n",
    "        meanstd = f\"Î¼={pretty_str(x.mean())} Ïƒ={pretty_str(x.std(ddof=ddof))}\" if x.size >= 2 else None\n",
    "        summary = sparse_join([minmax, meanstd])\n",
    "\n",
    "\n",
    "    return sparse_join([ summary, zeros])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "\n",
    "def to_str(x: jax.Array,  # Input\n",
    "            plain: bool=False,\n",
    "            verbose: bool=False,\n",
    "            depth=0,\n",
    "            lvl=0,\n",
    "            color=None) -> str:\n",
    "\n",
    "    if plain:\n",
    "        return plain_repr(x)\n",
    "\n",
    "    conf = get_config()\n",
    "\n",
    "    tname = type(x).__name__.split(\".\")[-1]\n",
    "    if tname in (\"ArrayImpl\"): tname = \"Array\"\n",
    "    shape = str(list(x.shape)) if x.ndim else None\n",
    "    type_str = sparse_join([tname, shape], sep=\"\")\n",
    "\n",
    "    # Check for sharding first, as sharded arrays also have .devices()\n",
    "    if hasattr(x, \"sharding\"):\n",
    "        dev = format_sharding(x.sharding)\n",
    "    elif hasattr(x, \"devices\"): # Unified Array (jax >= 0.4)\n",
    "        int_dev_ids = sorted([d.id for d in x.devices()])\n",
    "        ids = \",\".join(map(str, int_dev_ids))\n",
    "        dev = f\"{list(x.devices())[0].platform}:{ids}\"\n",
    "    elif hasattr(x, \"device\"): # Old-style DeviceArray\n",
    "        dev = f\"{x.device().platform}:{x.device().id}\"\n",
    "    else:\n",
    "        assert 0, f\"Weird input type={type(input)}, expecrted Array, DeviceArray, or ShardedDeviceArray\"\n",
    "\n",
    "    dtype = short_dtype(x)\n",
    "    # grad_fn = t.grad_fn.name() if t.grad_fn else None\n",
    "    # PyTorch does not want you to know, but all `grad_fn``\n",
    "    # tensors actuall have `requires_grad=True`` too.\n",
    "    # grad = \"grad\" if t.requires_grad else None\n",
    "    grad = grad_fn = None\n",
    "\n",
    "    # For complex tensors, just show the shape / size part for now.\n",
    "    if not jnp.iscomplexobj(x):\n",
    "        if color is None: color=conf.color\n",
    "        if in_debugger(): color = False\n",
    "        # `lovely-numpy` is used to calculate stats when doing so on GPU would require\n",
    "        # memory allocation (not float tensors, tensors with bad numbers), or if the\n",
    "        # data is on CPU (because numpy is faster).\n",
    "        #\n",
    "        # Temporarily set the numpy config to match our config for consistency.\n",
    "        with lnp_config(precision=conf.precision,\n",
    "                        threshold_min=conf.threshold_min,\n",
    "                        threshold_max=conf.threshold_max,\n",
    "                        sci_mode=conf.sci_mode):\n",
    "\n",
    "            if is_cpu(x) or is_nasty(x):\n",
    "                common = np_to_str_common(np.array(x), color=color)\n",
    "            else:\n",
    "                common = jax_to_str_common(x, color=color)\n",
    "\n",
    "            numel = None\n",
    "            if x.shape and max(x.shape) != x.size:\n",
    "                numel = f\"n={x.size}\"\n",
    "                if get_config().show_mem_above <= x.nbytes:\n",
    "                    numel = sparse_join([numel, f\"({bytes_to_human(x.nbytes)})\"])\n",
    "            elif get_config().show_mem_above <= x.nbytes:\n",
    "                numel = bytes_to_human(x.nbytes)\n",
    "\n",
    "            vals = pretty_str(x) if 0 < x.size <= 10 else None\n",
    "            res = sparse_join([type_str, dtype, numel, common, grad, grad_fn, dev, vals])\n",
    "    else:\n",
    "        res = plain_repr(x)\n",
    "\n",
    "    if verbose:\n",
    "        res += \"\\n\" + plain_repr(x)\n",
    "\n",
    "    if depth and x.ndim > 1:\n",
    "        with config(show_mem_above=jnp.inf):\n",
    "            deep_width = min((x.shape[0]), conf.deeper_width) # Print at most this many lines\n",
    "            deep_lines = [ \" \"*conf.indent*(lvl+1) + to_str(x[i,:], depth=depth-1, lvl=lvl+1, color=color)\n",
    "                                for i in range(deep_width)]\n",
    "\n",
    "            # If we were limited by width, print ...\n",
    "            if deep_width < x.shape[0]: deep_lines.append(\" \"*conf.indent*(lvl+1) + \"...\")\n",
    "\n",
    "            res += \"\\n\" + \"\\n\".join(deep_lines)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exporti\n",
    "def history_warning():\n",
    "    \"Issue a warning (once) ifw e are running in IPYthon with output cache enabled\"\n",
    "\n",
    "    if \"get_ipython\" in globals() and get_ipython().cache_size > 0:\n",
    "        warnings.warn(\"IPYthon has its output cache enabled. See https://xl0.github.io/lovely-tensors/history.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101203/3648473780.py:6: UserWarning: IPYthon has its output cache enabled. See https://xl0.github.io/lovely-tensors/history.html\n",
      "  warnings.warn(\"IPYthon has its output cache enabled. See https://xl0.github.io/lovely-tensors/history.html\")\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "get_ipython().cache_size=1000\n",
    "history_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "get_ipython().cache_size=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "class StrProxy():\n",
    "    def __init__(self, x: jax.Array, plain=False, verbose=False, depth=0, lvl=0, color=None):\n",
    "        self.x = x\n",
    "        self.plain = plain\n",
    "        self.verbose = verbose\n",
    "        self.depth=depth\n",
    "        self.lvl=lvl\n",
    "        self.color=color\n",
    "        history_warning()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return to_str(self.x, plain=self.plain, verbose=self.verbose,\n",
    "                      depth=self.depth, lvl=self.lvl, color=self.color)\n",
    "\n",
    "    # This is used for .deeper attribute and .deeper(depth=...).\n",
    "    # The second onthe results in a __call__.\n",
    "    def __call__(self, depth=1):\n",
    "        return StrProxy(self.x, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def lovely(x: jax.Array, # Tensor of interest\n",
    "            verbose=False,  # Whether to show the full tensor\n",
    "            plain=False,    # Just print if exactly as before\n",
    "            depth=0,        # Show stats in depth\n",
    "            color=None):    # Force color (True/False) or auto.\n",
    "    return StrProxy(x, verbose=verbose, plain=plain, depth=depth, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array cpu:0 1.623\n",
      "Array[2] Î¼=1.824 Ïƒ=0.201 cpu:0 [1.623, 2.025]\n",
      "Array[2, 3] n=6 xâˆˆ[-0.972, 2.025] Î¼=0.390 Ïƒ=1.080 cpu:0 [[1.623, 2.025, -0.434], [-0.079, 0.176, -0.972]]\n",
      "Array[11] xâˆˆ[-0.972, 2.180] Î¼=0.385 Ïƒ=1.081 cpu:0\n"
     ]
    }
   ],
   "source": [
    "print(lovely(randoms[0]))\n",
    "print(lovely(randoms[:2]))\n",
    "print(lovely(randoms[:6].reshape((2, 3)))) # More than 2 elements -> show statistics\n",
    "print(lovely(randoms[:11]))           # More than 10 -> suppress data output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_array_repr(str(lovely(randoms[0])),                \"Array cpu:0 1.623\")\n",
    "test_array_repr(str(lovely(randoms[:2])),               \"Array[2] Î¼=1.824 Ïƒ=0.201 cpu:0 [1.623, 2.025]\")\n",
    "test_array_repr(str(lovely(randoms[:6].reshape(2, 3))), \"Array[2, 3] n=6 xâˆˆ[-0.972, 2.025] Î¼=0.390 Ïƒ=1.080 cpu:0 [[1.623, 2.025, -0.434], [-0.079, 0.176, -0.972]]\")\n",
    "test_array_repr(str(lovely(randoms[:11])),              \"Array[11] xâˆˆ[-0.972, 2.180] Î¼=0.385 Ïƒ=1.081 cpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array f16 cpu:0 1.000\n",
      "Array f16 cpu:0 2.000\n"
     ]
    }
   ],
   "source": [
    "grad = jnp.array(1., dtype=jnp.float16)\n",
    "print(lovely(grad)); print(lovely(grad+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# test_eq(str(lovely(grad)), \"tensor f64 grad 1.000\")\n",
    "# test_eq(str(lovely(grad+1)), \"tensor f64 grad AddBackward0 2.000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     print(lovely(torch.tensor(1., device=torch.device(\"cuda:0\"))))\n",
    "#     test_eq(str(lovely(torch.tensor(1., device=torch.device(\"cuda:0\")))), \"tensor cuda:0 1.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have __any__ floating point nasties? Is the tensor __all__ zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[2, 6] n=12 xâˆˆ[-1.955, 1.623e+04] Î¼=1.803e+03 Ïƒ=5.099e+03 \u001b[31m+Inf!\u001b[0m \u001b[31m-Inf!\u001b[0m \u001b[31mNaN!\u001b[0m cpu:0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistics and range are calculated on good values only, if there are at lest 3 of them.\n",
    "lovely(spicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_array_repr(str(lovely(spicy)),\n",
    "    'Array[2, 6] n=12 xâˆˆ[-1.955, 1.623e+04] Î¼=1.803e+03 Ïƒ=5.099e+03 \\x1b[31m+Inf!\\x1b[0m \\x1b[31m-Inf!\\x1b[0m \\x1b[31mNaN!\\x1b[0m gpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[2, 6] n=12 xâˆˆ[-1.955, 1.623e+04] Î¼=1.803e+03 Ïƒ=5.099e+03 +Inf! -Inf! NaN! cpu:0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(spicy, color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Array[11] \\x1b[31mNaN!\\x1b[0m cpu:0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(lovely(jnp.array([float(\"nan\")]*11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_array_repr(str(lovely(jnp.array([float(\"nan\")]*11))),\n",
    "        'Array[11] \\x1b[31mNaN!\\x1b[0m gpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[12] \u001b[38;2;127;127;127mall_zeros\u001b[0m cpu:0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(jnp.zeros(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_array_repr(str(lovely(jnp.zeros(12))),\n",
    "        'Array[12] \\x1b[38;2;127;127;127mall_zeros\\x1b[0m gpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[0, 0, 0] f16 \u001b[38;2;127;127;127mempty\u001b[0m cpu:0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(jnp.array([], dtype=jnp.float16).reshape((0,0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_array_repr(str(lovely(jnp.array([], dtype=jnp.float16).reshape((0,0,0)))),\n",
    "        'Array[0, 0, 0] f16 \\x1b[38;2;127;127;127mempty\\x1b[0m gpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[3] i32 xâˆˆ[1, 3] Î¼=2.000 Ïƒ=0.816 cpu:0 [1, 2, 3]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(jnp.array([1,2,3], dtype=jnp.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "test_array_repr(str(lovely(jnp.array([1,2,3], dtype=jnp.int32))),\n",
    "        'Array[3] i32 xâˆˆ[1, 3] Î¼=2.000 Ïƒ=0.816 gpu:0 [1, 2, 3]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[2, 6] n=12 xâˆˆ[-1.955, 1.623e+04] Î¼=1.803e+03 Ïƒ=5.099e+03 \u001b[31m+Inf!\u001b[0m \u001b[31m-Inf!\u001b[0m \u001b[31mNaN!\u001b[0m cpu:0\n",
       "Array([[ 1.62e+04,  2.03e-04, -4.34e-01,       inf,      -inf,       nan],\n",
       "       [-4.95e-01,  4.94e-01,  6.64e-01, -9.50e-01,  2.18e+00, -1.96e+00]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.set_printoptions(linewidth=120, precision=2)\n",
    "lovely(spicy, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.6226422e+04,  2.0252647e-04, -4.3359444e-01,            inf,\n",
       "                  -inf,            nan],\n",
       "       [-4.9529874e-01,  4.9437860e-01,  6.6434932e-01, -9.5016348e-01,\n",
       "         2.1795304e+00, -1.9551506e+00]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lovely(spicy, plain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array[3, 196, 196] n=115248 (0.4Mb) xâˆˆ[-2.118, 2.640] Î¼=-0.388 Ïƒ=1.073 \u001b[31mNaN!\u001b[0m cpu:0\n",
       "  Array[196, 196] n=38416 xâˆˆ[-2.118, 2.249] Î¼=-0.324 Ïƒ=1.036 cpu:0\n",
       "    Array[196] xâˆˆ[-1.912, 2.249] Î¼=-0.673 Ïƒ=0.521 cpu:0\n",
       "    Array[196] xâˆˆ[-1.861, 2.163] Î¼=-0.738 Ïƒ=0.417 cpu:0\n",
       "    Array[196] xâˆˆ[-1.758, 2.198] Î¼=-0.806 Ïƒ=0.396 cpu:0\n",
       "    Array[196] xâˆˆ[-1.656, 2.249] Î¼=-0.849 Ïƒ=0.368 cpu:0\n",
       "    Array[196] xâˆˆ[-1.673, 2.198] Î¼=-0.857 Ïƒ=0.356 cpu:0\n",
       "    Array[196] xâˆˆ[-1.656, 2.146] Î¼=-0.848 Ïƒ=0.371 cpu:0\n",
       "    Array[196] xâˆˆ[-1.433, 2.215] Î¼=-0.784 Ïƒ=0.396 cpu:0\n",
       "    Array[196] xâˆˆ[-1.279, 2.249] Î¼=-0.695 Ïƒ=0.485 cpu:0\n",
       "    Array[196] xâˆˆ[-1.364, 2.249] Î¼=-0.637 Ïƒ=0.538 cpu:0\n",
       "    ...\n",
       "  Array[196, 196] n=38416 xâˆˆ[-1.966, 2.429] Î¼=-0.274 Ïƒ=0.973 \u001b[31mNaN!\u001b[0m cpu:0\n",
       "    Array[196] xâˆˆ[-1.861, 2.411] Î¼=-0.529 Ïƒ=0.555 cpu:0\n",
       "    Array[196] xâˆˆ[-1.826, 2.359] Î¼=-0.562 Ïƒ=0.472 cpu:0\n",
       "    Array[196] xâˆˆ[-1.756, 2.376] Î¼=-0.622 Ïƒ=0.458 \u001b[31mNaN!\u001b[0m cpu:0\n",
       "    Array[196] xâˆˆ[-1.633, 2.429] Î¼=-0.664 Ïƒ=0.429 cpu:0\n",
       "    Array[196] xâˆˆ[-1.651, 2.376] Î¼=-0.669 Ïƒ=0.398 cpu:0\n",
       "    Array[196] xâˆˆ[-1.633, 2.376] Î¼=-0.701 Ïƒ=0.390 cpu:0\n",
       "    Array[196] xâˆˆ[-1.563, 2.429] Î¼=-0.670 Ïƒ=0.379 cpu:0\n",
       "    Array[196] xâˆˆ[-1.475, 2.429] Î¼=-0.616 Ïƒ=0.385 cpu:0\n",
       "    Array[196] xâˆˆ[-1.511, 2.429] Î¼=-0.593 Ïƒ=0.398 cpu:0\n",
       "    ...\n",
       "  Array[196, 196] n=38416 xâˆˆ[-1.804, 2.640] Î¼=-0.567 Ïƒ=1.178 cpu:0\n",
       "    Array[196] xâˆˆ[-1.717, 2.396] Î¼=-0.982 Ïƒ=0.349 cpu:0\n",
       "    Array[196] xâˆˆ[-1.752, 2.326] Î¼=-1.034 Ïƒ=0.313 cpu:0\n",
       "    Array[196] xâˆˆ[-1.648, 2.379] Î¼=-1.086 Ïƒ=0.313 cpu:0\n",
       "    Array[196] xâˆˆ[-1.630, 2.466] Î¼=-1.121 Ïƒ=0.304 cpu:0\n",
       "    Array[196] xâˆˆ[-1.717, 2.448] Î¼=-1.120 Ïƒ=0.301 cpu:0\n",
       "    Array[196] xâˆˆ[-1.717, 2.431] Î¼=-1.166 Ïƒ=0.313 cpu:0\n",
       "    Array[196] xâˆˆ[-1.560, 2.448] Î¼=-1.124 Ïƒ=0.325 cpu:0\n",
       "    Array[196] xâˆˆ[-1.421, 2.431] Î¼=-1.064 Ïƒ=0.382 cpu:0\n",
       "    Array[196] xâˆˆ[-1.526, 2.396] Î¼=-1.047 Ïƒ=0.416 cpu:0\n",
       "    ..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = jnp.load(\"mysteryman.npy\")\n",
    "image = image.at[1,2,3].set(float('nan'))\n",
    "\n",
    "lovely(image, depth=2) # Limited by set_config(deeper_lines=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "#### CUDA memory is not leaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# |eval: false\n",
    "# def memstats():\n",
    "#     allocated = int(torch.cuda.memory_allocated() // (1024*1024))\n",
    "#     max_allocated = int(torch.cuda.max_memory_allocated() // (1024*1024))\n",
    "#     return f\"Allocated: {allocated} MB, Max: {max_allocated} Mb\"\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     cudamem = torch.cuda.memory_allocated()\n",
    "#     print(f\"before allocation: {memstats()}\")\n",
    "#     numbers = torch.randn((3, 1024, 1024), device=\"cuda\") # 12Mb image\n",
    "#     torch.cuda.synchronize()\n",
    "\n",
    "#     print(f\"after allocation: {memstats()}\")\n",
    "#     # Note, the return value of lovely() is not a string, but a\n",
    "#     # StrProxy that holds reference to 'numbers'. You have to del\n",
    "#     # the references to it, but once it's gone, the reference to\n",
    "#     # the tensor is gone too.\n",
    "#     display(lovely(numbers) )\n",
    "#     print(f\"after repr: {memstats()}\")\n",
    "\n",
    "#     del numbers\n",
    "#     # torch.cuda.memory.empty_cache()\n",
    "\n",
    "#     print(f\"after cleanup: {memstats()}\")\n",
    "#     test_eq(cudamem >= torch.cuda.memory_allocated(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.4011-0.4035j,  1.13  +0.0788j, -0.0277+0.9978j, -0.4636+0.6064j,\n",
       "       -1.1505-0.9865j], dtype=complex64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't really supposed complex numbers yet\n",
    "c = jnp.array([-0.4011-0.4035j,  1.1300+0.0788j, -0.0277+0.9978j, -0.4636+0.6064j, -1.1505-0.9865j])\n",
    "lovely(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: NamedSharding with 2D mesh (4,2) and P('y', 'x') ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   CPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">   CPU 1    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">   CPU 2    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   CPU 3    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">   CPU 4    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">   CPU 5    </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">   CPU 6    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">   CPU 7    </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m   \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m   \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m   \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m    \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m   \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m    \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m   \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m    \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m   \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m    \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array[8192, 8192] n=67108864 (0.2Gb) xâˆˆ[-5.420, 5.220] Î¼=1.508e-05 Ïƒ=1.000 S[y,x] 4Ã—2 cpu:0-7\n",
      "\n",
      "=== Test 2: NamedSharding with P('y', None) - replicate first dim ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         CPU 0,1         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         CPU 2,3         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         CPU 4,5         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">                         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">                         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         CPU 6,7         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">                         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,1\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;222;158;214m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 2,3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;222;158;214m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mCPU 4,5\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;173;73;74m                         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;181;207;107m                         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mCPU 6,7\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;181;207;107m                         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array[8192, 8192] n=67108864 (0.2Gb) xâˆˆ[-5.420, 5.220] Î¼=1.508e-05 Ïƒ=1.000 S[y,Â·] 4Ã—2 cpu:0-7\n",
      "\n",
      "=== Test 3: NamedSharding with P(None, 'x') - replicate second dim ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                        </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                        </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                        </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                        </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                        </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">CPU 0,2,4,6 CPU 1,3,5,7 </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                        </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                        </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                        </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                        </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                        </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,2,4,6\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 1,3,5,7\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array[8192, 8192] n=67108864 (0.2Gb) xâˆˆ[-5.420, 5.220] Î¼=1.508e-05 Ïƒ=1.000 S[Â·,x] 4Ã—2 cpu:0-7\n",
      "\n",
      "=== Test 4: 1D mesh with 8 devices ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">          CPU 0          </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">          CPU 1          </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">          CPU 2          </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">          CPU 3          </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">                         </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">          CPU 4          </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">          CPU 5          </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">          CPU 6          </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">                         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">          CPU 7          </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">                         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m          \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;214;97;107m          \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m          \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;214;97;107m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m          \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m          \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;162;82m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;222;158;214m          \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m          \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;222;158;214m                         \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m          \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m          \u001b[0m\n",
       "\u001b[38;2;0;0;0;48;2;231;203;148m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;107;110;207m          \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m          \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;107;110;207m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m          \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m          \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;165;81;148m                         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;109;49m          \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m          \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;140;109;49m                         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array[8192, 8192] n=67108864 (0.2Gb) xâˆˆ[-5.420, 5.220] Î¼=1.508e-05 Ïƒ=1.000 S[x,Â·] cpu:0-7\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "assert jax.__version_info__[0] == 0\n",
    "from jax.sharding import NamedSharding, Mesh, PartitionSpec as P\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "print(\"=== Test 1: NamedSharding with 2D mesh (4,2) and P('y', 'x') ===\")\n",
    "devices = mesh_utils.create_device_mesh((4, 2))\n",
    "mesh = Mesh(devices, axis_names=('y', 'x'))  # x has 4 devices, y has 2\n",
    "sharding = NamedSharding(mesh, P('y', 'x'))  # Shard array dim 0 across y, dim 1 across x\n",
    "\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (8192, 8192))\n",
    "y = jax.device_put(x, sharding)\n",
    "\n",
    "jax.debug.visualize_array_sharding(y)\n",
    "print(lovely(y))\n",
    "\n",
    "print(\"\\n=== Test 2: NamedSharding with P('y', None) - replicate first dim ===\")\n",
    "sharding2 = NamedSharding(mesh, P('y', None))\n",
    "y2 = jax.device_put(x, sharding2)\n",
    "jax.debug.visualize_array_sharding(y2)\n",
    "print(lovely(y2))\n",
    "\n",
    "print(\"\\n=== Test 3: NamedSharding with P(None, 'x') - replicate second dim ===\")\n",
    "sharding3 = NamedSharding(mesh, P(None, 'x'))\n",
    "y3 = jax.device_put(x, sharding3)\n",
    "jax.debug.visualize_array_sharding(y3)\n",
    "print(lovely(y3))\n",
    "\n",
    "print(\"\\n=== Test 4: 1D mesh with 8 devices ===\")\n",
    "devices_1d = mesh_utils.create_device_mesh((8,))\n",
    "mesh_1d = Mesh(devices_1d, axis_names=('x',))\n",
    "sharding_1d = NamedSharding(mesh_1d, P('x', None))\n",
    "y4 = jax.device_put(x, sharding_1d)\n",
    "jax.debug.visualize_array_sharding(y4)\n",
    "print(lovely(y4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
